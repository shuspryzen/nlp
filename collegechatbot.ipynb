{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994d47f-b94c-455b-a095-0ffa0c73d8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "College Enquiry Chatbot: Hello! How can I assist you today?\n",
      "Type 'exit' to end the chat.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Yes\n",
      "- Please provide a clear, concise, and concise answer.\n",
      "- Admissions\n",
      "- Fees\n",
      "- Placements\n",
      "- Hostel facilities.\n",
      "Please provide a clear, concise, and concise answer. If the user greets you with \"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Admissions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Yes\n",
      "- Please provide a clear, concise, and concise answer. If the user greets you with \"\n",
      "User: Admissions\n",
      "Chatbot: Yes\n",
      "- Please provide a clear, concise, and concise answer. If the user greets you with\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Fees?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Yes\n",
      "- Please provide a clear, concise, and concise answer. If the user greets you with \"\n",
      "User: Admissions\n",
      "Chatbot: Yes\n",
      "- Please provide a clear, concise, and concise answer. If the user greets you with\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and the model (we'll use distilgpt2 for faster responses)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(prompt, max_new_tokens=50):\n",
    "    # Encode the prompt to tensor format\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate response using the model\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=max_new_tokens,  # Limit the response length\n",
    "        num_return_sequences=1, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,          # Enable sampling for diversity\n",
    "        temperature=0.7,         # Controls randomness (0.7 gives more focused output)\n",
    "        top_k=50,                # Limits the number of highest-probability tokens to consider\n",
    "        top_p=0.9                # Nucleus sampling (cumulative probability distribution)\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens into a response string\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response[len(prompt):].strip()  # Remove the input prompt from the output\n",
    "\n",
    "# System instructions for context and conversation management\n",
    "base_context = \"\"\"\n",
    "You are a helpful and friendly College Enquiry Chatbot. You provide detailed, accurate answers about:\n",
    "- Courses\n",
    "- Admissions\n",
    "- Fees\n",
    "- Scholarships\n",
    "- Placements\n",
    "- Hostel facilities.\n",
    "\n",
    "Please answer in a professional, informative tone. Avoid repeating the user's input, and give clear, direct answers. If the user greets you with \"Hi\", respond with a friendly greeting and offer assistance.\n",
    "\"\"\"\n",
    "\n",
    "# Chat loop for continuous conversation\n",
    "def chatbot():\n",
    "    conversation_history = f\"{base_context}\\n\"\n",
    "    print(\"College Enquiry Chatbot: Hello! How can I assist you today?\")\n",
    "    print(\"Type 'exit' to end the chat.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "\n",
    "        if user_input.lower() in [\"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Thank you for your time! Have a great day.\")\n",
    "            break\n",
    "        \n",
    "        # Add user's input to the conversation history and prompt\n",
    "        prompt = conversation_history + f\"User: {user_input}\\nChatbot:\"\n",
    "        \n",
    "        # Generate a response based on the input\n",
    "        response = generate_response(prompt)\n",
    "        \n",
    "        # Display the chatbot's response\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        \n",
    "        # Add the new exchange to the conversation history\n",
    "        conversation_history += f\"User: {user_input}\\nChatbot: {response}\\n\"\n",
    "\n",
    "# Start the chatbot\n",
    "chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
